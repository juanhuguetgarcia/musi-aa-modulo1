{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas\n",
    "\n",
    "En las sesiones anteriores hemos descrito a los algoritmos de aprendizaje automático supervisados como funciones de mapeo entre unas variables independientes de entrada, $X$, y una variable objetivo de salida $y$. El objetivo de estas funciones de mapeo, $\\hat{f}$, no es el de encontrar la función real subyacente a las observaciones, si no darnos una herramienta **útil** para predecir el valor esperado de la variable objetivo, $\\hat{y}$, dado un nuevo set de  observaciones.\n",
    "\n",
    "Hemos visto que tanto los modelos paramétricos (regresión lineal), como los no paramétricos (arbol de decisión), los modelos cambian su estado interno para ajustarse a los datos observados en base a la minimización de un coste asociado a las predicciones del modelo. Este proceso de aprendizaje, o entrenamiento, nos permite obtener el mejor estado interno para un modelo dado. Por ejemplo, en el caso de una regresión lineal, sería aquel modelo tipo $\\hat{y} = \\theta X$ donde $\\theta$ nos minimiza el error cuadrático medio sobre la muestra.\n",
    "\n",
    "Recordemos que nosotros no conocemos la función real a aproximar y, que dada la naturaleza p-dimensional de los datos, siendo $p$ el número de predictores, no será trivial averiguar la relación entre los predictores y la variable objetivo. Es natural entonces que nos preguntemos ¿es el modelo que he elegido?\n",
    "\n",
    "La única forma de responder a esta pregunta es utilizando un metodo de comparación directa. Entrenar varios tipos de modelos y evaluar el rendimiento de esos modelos en base a una métrica que nos permita compararlos en base a la **finalidad** del problema a resolver.\n",
    "\n",
    "En esta sesión trabajaremos sobre las métricas que nos permitiran evaluar y comparar distintos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Metricas para problemas de regression\n",
    "\n",
    "Las métricas más comunes en los problemas de regresión son el **error cuadrático** y el **error absoluto**, y sus distintas modificaciones.\n",
    "\n",
    "### 1.1.2 Errores cuadráticos\n",
    "\n",
    "El **error cuadrático (Squared Error)** de un valor predicho con respecto al valor real, se calcula cómo:\n",
    "\n",
    "$$ SE = \\sum_j\\left[f(X_{j}) - y_j\\right]^2$$\n",
    "\n",
    "\n",
    "**Error cuadrático medio (Mean Squared Error)** Da una idea del error de nuestras predicciones dando más peso a los errores grandes.\n",
    "\n",
    "$$ MSE = \\frac{1}{m}\\sum_j^m \\left[f(X_{j \\cdot}) - y_j\\right]^2 $$\n",
    "\n",
    "**Raiz del error cuadrático medio (Root Mean Square Error)** La raíz cuadrada del MSE produce el error de la raíz cuadrada de la media o la desviación de la raíz cuadrada media (RMSE o RMSD). Tiene las mismas unidades que la cantidad que estima. Para un estimador sin sesgo (bies), el RMSE es la raíz cuadrada de la varianza, es decir la desviación estandar.\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{m}\\sum_j^m \\left[f(X_{j \\cdot}) - y_j\\right]^2} $$\n",
    "\n",
    "\n",
    ">**Tip**\n",
    ">\n",
    ">A pesar de ser una de las métricas más utilizadas, tiene el inconveniente de ser sensible a los valores extremos (outliers). Cuando este comportamiento pueda suponer un problema, los **errores absolutos** pueden darnos una mejor medida de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as ##\n",
    "import numpy as ##\n",
    "import seaborn as ##\n",
    "import matplotlib.pyplot as ##\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')\n",
    "sns.set(rc={'figure.figsize':(12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Escribe una función que devuelva el MSE y el RMSE dados dos arrays de numpy\n",
    "\n",
    "def MSE(x1, x2):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Error of the two input vectors\n",
    "    \"\"\"\n",
    "    sq_error = ##\n",
    "    mean_sq_error = np.##\n",
    "    return mean_sq_error\n",
    "\n",
    "def RMSE(x1, x2):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Error of two vectors. Depends on the MSE function\n",
    "    \"\"\"\n",
    "    mse = MSE(x1, x2)\n",
    "    root_mse = np.##\n",
    "    return root_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utiliza la función del error cuadrático en las métricas de sklearn para crear la funcion de RMSE\n",
    "from sklearn.metrics import ##\n",
    "\n",
    "def RMSE_sk(y_true, y_pred):\n",
    "    return np.sqrt(##(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Errores absolutos\n",
    "\n",
    "El **error absoluto** (Absolute Error) se define cómo:\n",
    "\n",
    "$$ AE = \\sum_j \\left|f(X_{j \\cdot}) - y_j\\right| $$\n",
    "\n",
    "#### Error absoluto medio (Mean Absolute Error)\n",
    "\n",
    "Es más robusto a los valores extremos y su interpretabilidad es más alta que la del RMSE ya que también está en las unidades de la variable a predecir con la ventaja de que el dato no ha sufrido ninguna transformación.\n",
    "\n",
    "$$ MAE = \\frac{1}{m} \\sum_j^m \\left|f(X_{j \\cdot}) - y_j\\right| $$\n",
    "\n",
    "#### Error absoluto medio porcentual (Mean Average Percentage Error)\n",
    "\n",
    "A pesar de su simpleza, presenta varios inconvenientes a la hora de usarlo de forma práctica. Por ejemplo, **no puede usarse cuando el valor de referencia es 0**. Además, **si se usa para elegir métodos predictivos seleccionará de forma sistemática un metodo que prediga valores bajos.**\n",
    "[wiki](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)\n",
    "\n",
    "$$ MAPE = \\frac{1}{m} \\sum_j^m \\left|\\frac{f(X_{j \\cdot}) - y_j}{y_j}\\right| $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Escribe una función que devuelva el MAE y el MAPE dados dos arrays de numpy\n",
    "import numpy as np\n",
    "\n",
    "def MAE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns de MAE of the two input vectors\n",
    "    \"\"\"\n",
    "    return ##\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns de MAPE of two vectors.\n",
    "    \"\"\"\n",
    "    return ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Generalización\n",
    "\n",
    "Las dos **métricas** expuestas anteriormente pueden considerarse como **distancias entre el vector de valores reales y el predicho**. De hecho, el RMSE corresponde a la **distancia euclidiana**, también conocida como norma $l_2$ o $\\lVert{v}\\rVert_2$.\n",
    "\n",
    "Por otro lado, el MAE corresponde a la norma $l_1$ o $\\lVert{v}\\rVert_1$. A esta distancia se la conoce como **distancia de manhattan**, porque sólo se puede viajar de un bloque a otro de la ciudad a traves de calles ortogonales.\n",
    "\n",
    "De forma general, una norma $l_k$ o $\\lVert{v}\\rVert_k$ se calcula:\n",
    "\n",
    "$$\\lVert{v}\\rVert_k = \\left(|v_0|^k + ...+ |v_m|^k \\right)^\\frac{1}{k}$$\n",
    "\n",
    "El concepto de distancia será particularmente útil en los problemas de segmentación (aprendizaje no supervisado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Coeficiente de determinación ($R^2$)\n",
    "\n",
    "El coeficiente determina la calidad del modelo para replicar los resultados, y la proporción de variación de los resultados que puede explicarse por el modelo. El valor más alto obtenible será 1, aunque hay casos en los que puede presentar valores negativos. De forma intuitiva, $R^2$ compara el \"fit\" de nuestro modelo al de una linea recta horizontal. Dada una regresión lineal simple, un $R^2$ negativo sólo es posible cuando la ordenada en el origen o la pendiente están restringidas de forma que el mejor modelo es peor que una linea horizontal.\n",
    "\n",
    "Si representamos la **varianza de la variable dependiente** por $\\sigma^{2}$ y la **varianza residual** por $\\sigma _{r}^{2}$, el coeficiente de determinación viene dado por la siguiente ecuación:\n",
    "\n",
    "$$ R^{2}=1- \\frac{\\sigma_{r}^{2}}{\\sigma ^{2}}$$\n",
    "\n",
    "\n",
    "Siendo $\\hat{y}_i$ el valor predicho de la muestra i y $y_i$ el valor real, el $R^2$ estimado sobre $n_{\\text{muestras}}$ se define como:\n",
    "\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{\\text{samples}} - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\bar{y})^2}$$\n",
    "donde $$ \\bar{y} =  \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1} y_i.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crea una función que dados dos vectores calcule la función de R²\n",
    "def r_square(y_true, y_pred):\n",
    "    var_res = ((y_true - y_pred)**2).sum()\n",
    "    var_y = ((y_true - y_true.mean())**2).sum()\n",
    "    return 1 - var_res/var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utiliza la función implementada en sklearn y compara los resultados\n",
    "from sklearn.metrics import ##\n",
    "\n",
    "y_true = np.random.random(10)\n",
    "y_pred = np.random.random(10)\n",
    "\n",
    "if ##(y_true, y_pred) == r_square(y_true, y_pred):\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5.1 Consideraciones R²\n",
    "\n",
    "1. R² no puede determinar si los coeficientes y las predicciones tienen bies: Hay que checkear los residos --> Si observamos patrones en los plots de residuos es indicativo de un mal ajuste a pesar de un R2 elevado\n",
    "\n",
    "1. Cada vez que añadimos un predictor a un modelo, el R² aumenta aunque sea por suerte, pero nunca decrece. Por consiguiente, un modelo con muchos terminos puede parecer mejor simplemente por el hecho de tener más terminos. Para prevenir este efecto, podemos usar el **adjusted R²**, una versión modificada que se ajusta al número de predictores en el modelo. De ésta forma, el R² solo aumenta si el nuevo término mejora el modelo más que por mera suerte. Siempre es más bajo que el R²\n",
    "\n",
    "$$\\bar{R}^2 = 1 - \\frac{N-1}{N-k-1}(1-R^2)$$\n",
    "\n",
    "\n",
    "n – numero de observaciones\n",
    "\n",
    "k – numero de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### crea una función que devuelva el R² ajustado\n",
    "\n",
    "def adj_r2_score(model, y_true, y_hat):\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "    n = y_true.shape[0]\n",
    "    k = model.coef_.shape[0]\n",
    "    num = n - 1\n",
    "    den = n - k - 1\n",
    "    return 1 - ((num) / (den) * (1 - r2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## load linear regression ML model\n",
    "from sklearn.linear_model import ##\n",
    "\n",
    "## load the tool to construct polynomial features\n",
    "from sklearn.preprocessing import ##\n",
    "\n",
    "## load diabetes dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = ##\n",
    "poly = ##(2)\n",
    "X_poly = poly.fit_transform(data.data)\n",
    "\n",
    "LR = LinearRegression()\n",
    "y = data.target\n",
    "\n",
    "r2 = []\n",
    "r2_adj = []\n",
    "## create a for loop training and predicting the y and calculating the r2 and r2 adj for each number of\n",
    "## features\n",
    "for n_cols in range(1, X_poly.shape[1]):\n",
    "    X = X_poly[:, :n_cols]\n",
    "    LR.fit(##, ##)\n",
    "    y_pred = LR.predict(##)\n",
    "    r2.append(r2_score(y, y_pred))\n",
    "    r2_adj.append(adj_r2_score(LR, y, y_pred))\n",
    "\n",
    "plt.plot(r2, label='r2')\n",
    "plt.plot(r2_adj, label='r2_adj')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Metricas para problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando hablamos de problemas de clasificación, existe una gran variedad de métricas que nos permiten determinar cómo de bueno es nuestro clasificador. La elección de la métrica vendrá determinada por:\n",
    "\n",
    "1. El tipo de predicción: si es una clase o la **probabilidad** de pertenecer a una clase\n",
    "2. Si se trata de un problema balanceado o no\n",
    "\n",
    "### 1.2.1 Matriz de confusión\n",
    "\n",
    "Antes de empezar, consideremos el caso básico de una clasificación binaria en el que intentamos predecir una clase. Imagina que hemos entrenado un modelo y realizado una serie de predicciones. Es fácil entonces crear una tabla que contenga la siguiente información.\n",
    "\n",
    "|                      | Observación: Perro                | Observación: Gato                 |\n",
    "|--------------------- |:---------------------------------:|:---------------------------------:|\n",
    "| **Predicción Perro** |<span style='color:green'>25</span>|<span style='color:red'>3</span>   |\n",
    "| **Predicción Gato**  |<span style='color:red'>9</span>   |<span style='color:green'>19</span>|\n",
    "\n",
    "Esta tabla se conoce cómo matriz de confusión (en ingles, confusion matrix) y es facilmente extendible a problemas de clasificación de varias categorías.\n",
    "\n",
    "Lo que nos dice esta matriz de forma simplificada es:\n",
    "\n",
    "1. En total tenemos 25+3+9+19 = 56 muestras sobre las que hemos realizado predicciones\n",
    "1. De las 56, 25 + 9 = 34 pertenecen a la clase *perro* y 3+19 = 21 pertenecen a la clase *gato*\n",
    "1. De los 34 *perros*, hemos predicho bien la categoría en 25 casos y mal en 9.\n",
    "1. De los 21 *gatos*, hemos predicho bien la categoría gato en 19 casos y mal en 3\n",
    "\n",
    "|                         | Observation Positive     | Observation Negative    |\n",
    "|-------------------------|:------------------------:|:-----------------------:|\n",
    "| **Prediction Positive** |     True Positive        | False Positive (Type I) |\n",
    "| **Prediction Negative** | False Negative (Type II) |     True Negative       |\n",
    "\n",
    "A partir de esta matriz, se construyen la mayoría de métricas asociadas con los problemas de clasificación.\n",
    "\n",
    "### 1.2.2 Métricas según clase predicha\n",
    "**Accuracy**\n",
    "\n",
    "De forma general, ¿cuantas veces predigo la clase correcta? La más común y muchas veces, la más susceptible a darnos un clasificador erroneo, sobre todo en sets de datos no balanceados.\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{total} = \\frac{25+19}{56} \\sim 0.786$$\n",
    "\n",
    "**Error Rate**\n",
    "\n",
    "De similar modo, ¿Cuántas veces me equivoco?\n",
    "\n",
    "$$ Error Rate = 1 - Accuracy = \\frac{FP+FN}{total} = \\frac{9+3}{56} \\sim 0.214 $$\n",
    "\n",
    "**Recall (sensitivity)**\n",
    "\n",
    "Cuando es *perro*, ¿cuantas veces predecimos *perro*?\n",
    "\n",
    "$$ Recall = \\frac{TP}{actual Trues} = \\frac{TP}{TP + FN} = \\frac{25}{25 + 9} \\sim 0.735 $$\n",
    "\n",
    "**Specificity**\n",
    "\n",
    "Cuando es *gato*, ¿cuántas veces predecimos *gato*?\n",
    "\n",
    "$$ Specificity = \\frac{TN}{actual Falses} = \\frac{TN}{TN + FP} = \\frac{21}{19 + 9} \\sim 0.75 $$\n",
    "\n",
    "**Precision**\n",
    "\n",
    "Cuando predecimos *perro*, cuantas veces es correcto?\n",
    "\n",
    "$$ Precision = \\frac{TP}{predicted Trues} = \\frac{TP}{TP + FP} = \\frac{25}{25 + 3} \\sim 0.892 $$\n",
    "\n",
    "**F1-score**\n",
    "\n",
    "A menudo es conveniente **combinar la precision y el recall en una sola métrica** para comparar de forma sencilla dos clasificadores. En vez de calcular una media de la precisión y el recall, se calcula su **media harmónica**. De esta forma se da más peso a los valores bajos por lo que sólo se conseguirá un F1-score alto si ambas, Precision y Recall, son altas.\n",
    "\n",
    "$$F_1= \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 \\times \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}} $$\n",
    "\n",
    "\n",
    "Vamos a ver un ejemplo de estas métricas. Para ello, usaremos los datos de vinos que vienen por defecto en la libreria de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargamos los datos de vinos\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = load_wine()\n",
    "\n",
    "## Create dataframe with data.data\n",
    "df = pd.DataFrame(##)\n",
    "df.columns = data.feature_names\n",
    "df['y'] = data.target\n",
    "\n",
    "# Vamos a forzar solo que distinga entre la clase 0 y las demas\n",
    "# Asigna a la variable 'y' la clase 0 cuando sea 0 y 1 en caso contrario\n",
    "df['y'] = df['y'].where(##, ##)\n",
    "\n",
    "# importa el modulo de regresión logística\n",
    "from sklearn.linear_model import ##\n",
    "\n",
    "# importa el modulo de StratifiedKfold\n",
    "from sklearn.model_selection import ##\n",
    "\n",
    "# importa el modulo de cross_val_predict\n",
    "from sklearn.model_selection import ##\n",
    "\n",
    "#importa el modulo necesario para realizar la partición entre train y test\n",
    "from sklearn.model_selection import ##\n",
    "\n",
    "# divide el dataset en train y test con un tamaño de test del 10% de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(##, ##, test_size=0.1)\n",
    "\n",
    "# usa el StratifiedKFold para dividir el X_train en 5 partes.\n",
    "kfold = StratifiedKFold(n_splits=##)\n",
    "\n",
    "# instancia una regresión logistica\n",
    "clf = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "# usa la función de cross_val_predict para entrenar y evaluar la regresión logística usando el StratifiedKFold\n",
    "y_pred = cross_val_predict(clf, X_train, y_train, cv=##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos realizado las predicciones, veamos cómo se comporta calculando las métricas comentadas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, labels=['0', '1']):\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(##, annot=True, ax = ax, cmap='Blues')\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    if ##:\n",
    "        ax.xaxis.set_ticklabels(labels)\n",
    "        ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(\"Precision:\", precision_score(y_train, y_pred))\n",
    "print(\"Recall:\", recall_score(y_train, y_pred))\n",
    "print(\"F1:\", f1_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otra funcionalidad que nos da una visión panorámica de nuestro clasificador es...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Metricas para predicciones probabilisticas\n",
    "\n",
    "Muchas veces no solo nos conformaremos con predecir una clase, si no que tendremos que predecir la *probabilidad* de pertenecer a una clase.\n",
    "\n",
    "Varios métodos de clasificación de ML son aptos para este tipo de tarea. En sklear, además del método `clf.predict()` de algunos estimadores, tendremos el método `clf.predict_proba()`, que nos devolverá la probabilidad de pertenecer a cada una de las clases.\n",
    "\n",
    "En realidad, un clasificador calcula la probabilidad y, en base a un umbral, decide la clase a la que pertenece.\n",
    "Por defecto, los algoritmos de ML de sklearn tienen un umbral de 0.5. Es decir, si la probabilidad de pertenecer a una clase es mayor que 0.5, le asigna la etiqueta de esa clase.\n",
    "\n",
    "**¿Qué implica esto?**\n",
    "\n",
    "Básicamente, implica que si jugamos con el umbral de decisión para un predictor probabilístico, podemos alterar las métricas de performance a nuestra voluntad...\n",
    "\n",
    "### Precision vs recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "y = np.random.binomial(1, x)\n",
    "\n",
    "def plot_threshold(threshold=0.5):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    true_pos = (x > threshold) * (y > 0)\n",
    "    plt.plot(x[true_pos], y[true_pos], '.', label=\"True Positive\")\n",
    "    false_pos = (x > threshold) * (y == 0)\n",
    "    plt.plot(x[false_pos], y[false_pos], '.', label=\"False Positive\")\n",
    "    true_neg = (x <= threshold) * (y == 0)\n",
    "    plt.plot(x[true_neg], y[true_neg], '.', label=\"True Negative\")\n",
    "    false_neg = (x <= threshold) * (y > 0)\n",
    "    plt.plot(x[false_neg], y[false_neg], '.', label=\"False Negative\")\n",
    "    plt.axvline(threshold, c='k')\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.yticks([0,1])\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "    try:\n",
    "        precision = 1.0 * sum(true_pos) / (sum(true_pos) + sum(false_pos))\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1\n",
    "    recall = 1.0 * sum(true_pos) / (sum(true_pos) + sum(false_neg))\n",
    "    plt.title('Precision: %0.2f, Recall: %0.2f' % (precision, recall))\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_threshold, threshold=(0, 1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tiene sentido hablar de éstas métricas de forma independiente, ya que podemos alterar virtualmente su valor al modificar el umbral de decisión. Por ejemplo, si seteamos el umbral de decision muy alto de forma que el clasificador nos devuelva siempre la clase 0, nuestro clasificador tendrá una precision del 100% y un recall nulo.\n",
    "\n",
    "En el caso contrario, si el umbral lo ponemos de forma que facilmente prediga la clase positiva, nuestro clasificador tendrá un recall del 100% a expensas de la precisión.\n",
    "\n",
    "Este comportamiento no tiene por que ser negativo per se. Dependerá del tipo de problema que estemos tratando. \n",
    "\n",
    "**Por ejemplo, en un caso de detección de cancer...que es mejor? Una precision alta o un recall alto?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a predecir las probabilidades del ejercicio anterior, utiliza `predict_proba` en el argumento method\n",
    "y_proba = cross_val_predict(clf, X_train, y_train, cv=kfold, method=##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa el metodo que nos permite encontrar la curva de precisión y recall\n",
    "from sklearn.metrics import ##\n",
    "\n",
    "# calcula la precision, el recall y los umbrales\n",
    "prec, rec, thre = precision_recall_curve(y_true=y_train, probas_pred=y_proba[:,1])  # Nota: seleccionamos solo la segunda columna, corresponde a la clase 1\n",
    "\n",
    "# haz un plot de la precision y el recall en función de los umbrales\n",
    "plt.plot(thre, prec[:-1], label='precision')\n",
    "plt.plot(thre, rec[:-1], label='recall')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('threshold')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotea la precisión en funcion del recall\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "De forma similar a la curva de precision-recall, la curva ROC (*Receiver Operating Characteristic*) es otra herramienta típica usada en clasificadores binarios.\n",
    "\n",
    "A diferencia de la curva PR, la curva ROC compara el True Positive Rate (Recall) y el False Positive Rate (1 - specifity). Recuerda que la *specifity* es el True Negative Rate, el ratio de veces que la clase negativa se clasifica correctamente.\n",
    "\n",
    "Otra forma de comparar clasificadores es comparando el area bajo la curva ROC (Area Under the Curve, AUC). Éste parametro tiene cómo límite superior 1, mientras que un valor de 0.5 corresponde a un clasificador completamente aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repite el ejercicio anterior pero con la curva ROC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Nota: seleccionamos solo la segunda columna, corresponde a la clase 1 en y_proba\n",
    "fpr, tpr, thre = roc_curve(y_train, ##)\n",
    "roc_auc = roc_auc_score(y_train, ##)\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = {a}'.format(a=round(roc_auc, 3)))\n",
    "plt.plot([0, 1], '--', c='black', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que la curva ROC y la Prec vs Recall son muy parecidas, uno puede tener dudas al tener que elegir una de ellas. Cómo guía no exhaustiva, uno puede decantarse en función del problema que tenga. Concretamente, en casos no balanceados o cuando una clase nos importa más que la otra, la curva PR nos aportará más información que la curva ROC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Ejercicio\n",
    "\n",
    "En el fichero `data/credicard.csv` encontraremos un dataset sobre transacciones de tarjetas de credito. El objetivo es entrenar un modelo y evaluar su rendimiento para detectar los usos fraudulentos utilizando las métricas descritas anteriormente:\n",
    "\n",
    "1. Carga los datos en un df de pandas\n",
    "\n",
    "1. Comprueba rapidamente la distribucion de los datos (head, y describe)\n",
    "    1. Cómo sabemos que es un problema no balanceado ?\n",
    "    \n",
    "1. Separa el df en train y test con un 20% de los datos en test\n",
    "    1. Comprueba que y_train e y_test tienen una proporción similar de clases 0 y 1\n",
    "\n",
    "1. Inicializa una regresion logistica con 'liblinear' como solver\n",
    "1. Inicializa un Random Forest con `n_estimators=30`\n",
    "1. Inicializa un k-fold stratificado con un numero de splits = 3: `from sklearn.model_selection import StratifiedKFold`\n",
    "1. Utiliza la funcion `cross_val_predict` sobre el punto anterior y la regresion logística: `from sklearn.model_selection import cross_val_predict`\n",
    "1. Obtén los valores de las métricas de clasificación descritas anteriormente para evaluar los modelos. Qué modelo se comporta mejor ?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uib",
   "language": "python",
   "name": "uib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
