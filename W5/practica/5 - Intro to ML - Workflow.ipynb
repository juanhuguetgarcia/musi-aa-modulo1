{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML\n",
    "## Workflow: Pipelines and Feature Unions\n",
    "---\n",
    "\n",
    "En la última sesión comentamos qué métricas nos permiten discriminar los modelos y cómo calcularlas con especial enfasis en métricas de clasificación.\n",
    "\n",
    "En la sesión de hoy, haremos una mención especial a cómo tratar de forma programática las distintas fases de un trabajo o proyecto de modelización: transformación de los datos, afinado de hiper-parametros y validación.\n",
    "\n",
    "\n",
    "A final del notebook deberíais entender como usar:\n",
    "\n",
    "* Pipelines\n",
    "* Feature Unions\n",
    "* Custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipelines\n",
    "\n",
    "Al encarar un proyecto de modelizado de datos, antes de poder utilizar un motor de predicción o clasificación, tendremos que *jugar* con los datos con dos objetivos:\n",
    "\n",
    "1. Modificar/seleccionar las variables predictoras con tal de obtener las mejores predicciones\n",
    "1. Alimentar el algoritmo de ML con un formato de datos que sea capaz de utilizar\n",
    "\n",
    "Dicho de otra forma, los datos pasarán por una serie transformaciones y por último por un paso de modelizado.\n",
    "\n",
    "Cada uno de los pasos de la secuencia puede ser definido como un **componente** y el flujo de transformación como una serie de **componentes** encadenados.\n",
    "\n",
    "---\n",
    "\n",
    "<img src='img/pipeline.png'>\n",
    "\n",
    "---\n",
    "\n",
    "A este tipo de proceso, aplicable a cualquier tipo de transformacion de datos o ETL, se lo conoce como Grafo Aciclico Dirigido o `DAG` por sus siglas en inglés. Dicho de mejor forma:\n",
    "\n",
    ">**Data is immutable**\n",
    ">\n",
    ">Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw.\n",
    "> \n",
    "> [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/#data-is-immutable) - A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.\n",
    "\n",
    "Si bien los DAGs pueden llegar a tal complejidad que se compongan de distintos procesos que requieren un orquestador que se encargue de ejecutarlos y monitorizarlos (e.g. el orquestador más popular actualmente es [Airflow](https://airflow.apache.org/), `sklearn` nos provee de una interfaz para crear pipelines sencilla.\n",
    "\n",
    "Los `pipelines` de sklearn nos permiten encadenar los distintos transformers de sklearn, tanto de pre-procesado como de modelizado, de forma secuencial de forma que la salida de un componente sea la entrada del siguiente.\n",
    "\n",
    "Las principales ventajas serán:\n",
    "* Conveniencia y encapsulación: Solo se tendrán que utilizar los metodos `.fit` y `.predict` una vez dada la secuencia de estimadores.\n",
    "\n",
    "* Selección de parametros conjunta: Podremos realizar el GridSearch no solo sobre los hiperparametros del parametro si no tambien sobre las distintas etapas de transformación previa (p. ej. numero de componentes de PCA a usar)\n",
    "\n",
    "* Safety: Evita el \"information leakeage\" entre train y test aseugrandose que las mismas muestras son las que se usan en todas las etapas de transformación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos los datos con unos cuantos nan's..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "### adding some nan's\n",
    "mask = np.random.choice([0,1], p=[0.99, 0.01], size=X.shape).astype(np.bool)\n",
    "X[mask] = np.nan\n",
    "\n",
    "X = pd.DataFrame(X, columns=data['feature_names'])\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo transformaciones\n",
    "\n",
    "Los `transformers` de sklearn son aquellos que tienen los metodos de `fit_transform`. Podremos encadenar todos los que queramos siguiendo una secuencia de transformaciones.\n",
    "\n",
    "En este ejemplo vemos como podemos encadenar:\n",
    "\n",
    "1. Imputación de nulos\n",
    "2. Normalización\n",
    "\n",
    "Recordad que todas las transformaciones se ejecutarán sobre el subset `train`. De ésta forma, la media para imputación se calculará solo sobre el subset de entrenamiento y la usaremos para popular los nulos en el dataset de testeo. Esto se hace así para poder simular las condiciones de uso reales, donde las predicciones siempre serán \"out of sample\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## Dividimos los datos en test y train\n",
    "X_train, X_test, y_train, y_test = ###\n",
    "\n",
    "## Definimos los pasos del pipeline como tuplas (name, Transformer)\n",
    "imputation_step = ('imputer', SimpleImputer(strategy='mean'))\n",
    "scaling_step = ('scaler', StandardScaler())\n",
    "\n",
    "## Los ordenamos en una lista\n",
    "steps = [##, ##]\n",
    "\n",
    "## Finalmente llamamos al creador de pipeline\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_train_transformed = pipe.fit_transform(X_train)\n",
    "X_test_transformed = pipe.transform(X_test)\n",
    "\n",
    "print('X_train: \\n')\n",
    "print('Mean before pipeline: \\n', X_train.mean())\n",
    "print('Mean after pipeline: \\n', X_train_transformed.mean(axis=0))\n",
    "\n",
    "print('\\n X_test: \\n')\n",
    "print('Mean before pipeline: \\n', X_test.mean())\n",
    "print('Mean after pipeline: \\n', X_test_transformed.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podemos acceder a los valores asignados a los atributos de cada `step` del pipeline por su nombre. Estos tendran los atributos del `transformer` utilizado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores de imputación utilizados\n",
    "print(pipe['imputer'].statistics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search sobre parametros de los transformadores\n",
    "\n",
    "Una de las ventajas de usar un pipeline, es que podemos definir un gridsearch donde los parametros de busqueda incluyan aquellos de los transformadores. Vamos a ver si es mejor usar la media o la mediana..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import pipeline\n",
    "from sklearn.pipeline import ##\n",
    "\n",
    "## Import StandardScaler\n",
    "from sklearn.preprocessing import ##\n",
    "\n",
    "## Import SimpleImputer\n",
    "from sklearn.impute import ##\n",
    "\n",
    "## Import RandomForestClassifier\n",
    "from sklearn.ensemble import ##\n",
    "\n",
    "## Import GridSearch y train_test_split\n",
    "from sklearn.model_selection import ##\n",
    "from sklearn.model_selection import ##\n",
    "\n",
    "## Dividimos los datos en test y train\n",
    "X_train, X_test, y_train, y_test = ##\n",
    "\n",
    "## Definimos los pasos del pipeline como tuplas (name, Transformer)\n",
    "imputation_step = ('imputer', ##)\n",
    "scaling_step = ('scaler', ##)\n",
    "model_step = ('clf', ##)\n",
    "\n",
    "## Los ordenamos en una lista\n",
    "steps = [##, ##, ##]\n",
    "\n",
    "## Finalmente llamamos al creador de pipeline\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "\n",
    "## Definimos el espacio de busqueda para el gridsearch.\n",
    "## Podemos acceder a los distintos hiperparametros de los Transformadores como nombre__hiperparametro\n",
    "\n",
    "param_grid = {'imputer__strategy': ['mean', 'median'],\n",
    "              'clf__max_depth': [3, 5, 10, 15],\n",
    "              'clf__min_samples_leaf': [2, 5, 10, 15]}\n",
    "\n",
    "## usamos el pipe como estimador en el gridsearch\n",
    "clf_gs = GridSearchCV(##, cv=3, n_jobs=-1, param_grid=##, verbose=1)\n",
    "\n",
    "clf_gs.fit(X_train, y_train)\n",
    "\n",
    "## sleep 1 sec to avoid overprinting\n",
    "time.sleep(1)\n",
    "\n",
    "print('Best params: ', clf_gs.best_params_)\n",
    "print('Best score: ', clf_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para poder hacer predicciones, simplemente debemos llamar al clf_gs con el metodo predict.**\n",
    "**El pipeline se encarga de hacer las transformaciones necesarias antes de pasarle los datos al predictor.**\n",
    "Esto es ideal para la productivización de un modelo. Nos permite recoger el dato \"crudo\" y pasarlo por una sola clase donde se tengan en cuenta todas las transformaciones necesarias antes de pasarlo al modelo para hacer la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ##\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering usando Principal Component Analysis\n",
    "\n",
    "En la última sesión vimos como el PCA podía usarse tanto como una tranformación `many to many` para evitar la colinearidad de algunos predictores y como seleccion de variables para reducir la dimensionalidad. \n",
    "\n",
    "Realizar PCA sobre un set de datos multidimensional nos permitirá:\n",
    "\n",
    "1. Visualizar una proyección de los datos en 2-3 dimensiones\n",
    "1. Eliminar colinearidad\n",
    "1. Reducir el número de predictores: seleccionaremos un número límitado de componentes de forma que expliquemos la mayor parte de la varianza de los datos\n",
    "\n",
    "Por ejemplo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import ##\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "## Definimos los pasos del pipeline como tuplas (name, Transformer)\n",
    "scaling_step = ('scaler', ##)\n",
    "pca_step = ('pca', ##)\n",
    "\n",
    "## Los ordenamos en una lista\n",
    "steps = [##, ##]\n",
    "\n",
    "## Finalmente llamamos al creador de pipeline\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_pca = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax[0].scatter(X_pca[y==0][:,0], X_pca[y==0][:, 1], c='green', alpha=0.5, label='Not Cancer')\n",
    "ax[0].scatter(X_pca[y==1][:,0], X_pca[y==1][:, 1], c='firebrick', alpha=0.5, label='Cancer')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[0].set_title('2D Projection')\n",
    "\n",
    "ax[1].plot(pipe['pca'].explained_variance_ratio_.cumsum())\n",
    "ax[1].set_xlabel('Num of PCA components')\n",
    "ax[1].set_title('Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: Traduce el siguiente script a un pipeline y evalualo en el dataset de testeo.\n",
    "\n",
    "Nota: En vez de seleccionar manualmente el umbral de varianza explicada para el PCA, introduce el número de componentes en el gridsearch del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['y'] = data.target\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df.y,\n",
    "                                                    test_size = 0.33, random_state=123)\n",
    "###########################################\n",
    "## Transformation 1: Polynomial Features ##\n",
    "###########################################\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "# fit scaler to train data\n",
    "poly.fit(X_train)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly= poly.transform(X_train)\n",
    "\n",
    "#######################################\n",
    "## Transformation 2: feature scaling ##\n",
    "#######################################\n",
    "\n",
    "## It is important to only to fit the transformer on the train dataset!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit scaler to train data\n",
    "scaler.fit(X_train_poly)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly_scaled= scaler.transform(X_train_poly)\n",
    "\n",
    "###########################\n",
    "## Transformation 3: PCA ##\n",
    "###########################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# instantiate pca\n",
    "pca = PCA()\n",
    "\n",
    "# fit PCA to scaled train data\n",
    "pca.fit(X_train_poly_scaled)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly_scaled_pca = pca.transform(X_train_poly_scaled)\n",
    "\n",
    "#########################################\n",
    "## Transformation 4: Feature Selection ##\n",
    "#########################################\n",
    "\n",
    "# Num of components needed to reach 95%\n",
    "num_comp_95 = pca.explained_variance_ratio_[pca.explained_variance_ratio_.cumsum() < 0.95].shape[0]\n",
    "\n",
    "# instantiate pca with number of components desired\n",
    "pca = PCA(num_comp_95)\n",
    "\n",
    "# refit PCA to scaled train data\n",
    "pca.fit(X_train_poly_scaled)\n",
    "\n",
    "# scale X_train\n",
    "X_train_final = pca.transform(X_train_poly_scaled)\n",
    "\n",
    "##################\n",
    "## Modelization ##\n",
    "##################\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Define stratified kfold\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "## Instantiate Random Forest\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "## set up a parameter grid for the gridsearch\n",
    "params_clf_rf = {'max_depth': [2, 3, 4],\n",
    "                 'min_samples_leaf': [8, 9, 10, 11, 12],\n",
    "                 'min_samples_split': [3, 4, 5, 6, 7]}\n",
    "\n",
    "## Instantiate the gridsearch\n",
    "clf_gs = GridSearchCV(estimator=clf_rf, cv=skfold, param_grid=params_clf_rf, verbose=1, n_jobs=-1)\n",
    "\n",
    "## Fit the gridsearch\n",
    "clf_gs.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## Transformation 1: Polynomial Features ##\n",
    "###########################################\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate polynomial features\n",
    "step_poly = ##\n",
    "\n",
    "#######################################\n",
    "## Transformation 2: feature scaling ##\n",
    "#######################################\n",
    "\n",
    "from sklearn.preprocessing import ##\n",
    "\n",
    "step_scaler = ##\n",
    "\n",
    "###########################\n",
    "## Transformation 3: PCA ##\n",
    "###########################\n",
    "\n",
    "from sklearn.decomposition import ##\n",
    "\n",
    "step_pca = ##\n",
    "\n",
    "##################\n",
    "## Modelization ##\n",
    "##################\n",
    "\n",
    "from sklearn.ensemble import ##\n",
    "\n",
    "model_step = ##\n",
    "\n",
    "##########################\n",
    "## Make pipe            ##\n",
    "##########################\n",
    "from sklearn.pipeline import ##\n",
    "\n",
    "pipe_steps = [##, ##, ##, ##]\n",
    "pipe = Pipeline(pipe_steps)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Define stratified kfold\n",
    "skfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "\n",
    "params_pipe = {'poly__degree': [2, 3],\n",
    "               'pca__n_components': [9, 10, 11],\n",
    "               'clf__max_depth': [3, 4],\n",
    "               'clf__min_samples_leaf': [8, 9, 10],\n",
    "               'clf__min_samples_split': [5, 6, 7]}\n",
    "\n",
    "## Instantiate the gridsearch\n",
    "clf_gs = GridSearchCV(pipe, cv=skfold, param_grid=params_pipe, verbose=1, n_jobs=-1)\n",
    "\n",
    "## Fit the gridsearch\n",
    "clf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_gs.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature unions\n",
    "\n",
    "Hasta ahora, solo hemos trabajado con un tipo de variables: númericas. Sin embargo, si tuvieramos variables categóricas, deberíamos tratarlas de forma distinta. Por ejemplo, haciendo un **OneHotEncoding**.\n",
    "\n",
    "Las feature unions nos permiten trabajar con transformaciones según el tipo de variable, para despues unirlas antes de pasarlas al modelo de ML.\n",
    "\n",
    "![feat-union](img/pipeline+featunion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos el dataset con el que trabajaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/abalone.csv',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers\n",
    "\n",
    "Con tal de poder interactuar con los `Pipelines`, sklearn nos da la opcion de crear nuestras propias transformaciones basandonos en su sintaxis.\n",
    "\n",
    "Para esto crearemos una clase que herede el `TransformerMixin`. Al heredar esta clase, si creamos una que tenga los metodos `fit` y `transform` nos dará automaticamente un metodo que sea `fit_transform` y que sea compatible con el resto de la `suite` de sklearn.\n",
    "\n",
    "Vamos a crear un `transformer` propio que nos devuelva las columnas elegidas durante la inicialización. Usaremos este `transformer` como primera etapa en el pipe de OHE para seleccionar la columna categorica\n",
    "\n",
    "Vamos a crear otro `transformer` que nos haga lo contrario, que nos elimine la columna especificada.\n",
    "\n",
    "De esta forma, podremos usar un solo punto de entrada al pipeline y será el pipeline el que se encargará de hacer la division entre variables numericas y categoricas, como en la imagen anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "### aux functions\n",
    "\n",
    "class SelectColumns(TransformerMixin):\n",
    "    def __init__(self, columns: list) -> pd.DataFrame:\n",
    "        if not isinstance(columns, list):\n",
    "            raise ValueError('Specify the columns into a list')\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None): # we do not need to specify the target in the transformer. We leave it as optional arg for consistency\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n",
    "    \n",
    "class DropColumns(TransformerMixin):\n",
    "    def __init__(self, columns: list) -> pd.DataFrame:\n",
    "        if not isinstance(columns, list):\n",
    "            raise ValueError('Specify the columns into a list')\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(self.columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = SelectColumns(['sex'])\n",
    "sel_col.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = DropColumns(['sex'])\n",
    "drop_col.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transformation to one-hot-encode 'sex' feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "select_col_step = ('select', SelectColumns(##))\n",
    "\n",
    "one_hot_step = ('sex_one_hot', OneHotEncoder(sparse=False))\n",
    "\n",
    "cat_pipe_steps = [select_col_step, one_hot_step]\n",
    "\n",
    "cat_pipe = Pipeline(cat_pipe_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe['sex_one_hot'].categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation of the numeric values by MinMaxScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "drop_column_step = ('drop_column', ##)\n",
    "\n",
    "poly_step = ('poly', PolynomialFeatures(2,  interaction_only=True))\n",
    "\n",
    "scaler_step = ('scaler', MinMaxScaler())\n",
    "\n",
    "num_pipe_steps = [#, #, #]\n",
    "\n",
    "num_pipe = Pipeline(num_pipe_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the two pipes using feature union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "transformer_list = [('num_pipe', num_pipe),\n",
    "                    ('cat_pipe', cat_pipe)]\n",
    "\n",
    "full_pipe = FeatureUnion(transformer_list=transformer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `full_pipe` has the parallel pipeline represented in the image above.\n",
    "\n",
    "We can use it directly as a transformer to check the results or use it in a new pipeline as a preprocessing step before modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it as a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('y_rings', axis=1), data.y_rings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = full_pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = full_pipe.transform(##)\n",
    "y_test_pred_pipe = reg.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_test_pred_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = y_test.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, alpha=0.5, bins=bins, label='Real')\n",
    "plt.hist(y_test_pred_pipe, alpha=0.5, bins=bins, label='Predicted')\n",
    "plt.xlabel('rings')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.scatter(y_test, y_test_pred_pipe)\n",
    "ax.set_xlabel('Real Value')\n",
    "ax.set_ylabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: Repite el ejercicio anterion añadiendo el random forest al pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numerical columns\n",
    "\n",
    "drop_column_step = #\n",
    "poly_step = #\n",
    "scaler_step = #\n",
    "num_pipe_steps = #\n",
    "num_pipe = #\n",
    "\n",
    "## Categorical columns\n",
    "\n",
    "select_col_step = #\n",
    "one_hot_step = #\n",
    "cat_pipe_steps = #\n",
    "cat_pipe = #\n",
    "\n",
    "## Modelling step\n",
    "\n",
    "regressor_step = #\n",
    "\n",
    "## Compose Pipeline\n",
    "\n",
    "### Feature uninon over numerical and categorical transformation\n",
    "\n",
    "transformer_list = ##\n",
    "\n",
    "data_prep_pipe = ##\n",
    "data_prep_step = ##\n",
    "\n",
    "### Compose full pipe: RandomForest over data_prep_pipe\n",
    "\n",
    "pipe_steps = ##\n",
    "\n",
    "pipe = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Review raw data\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train all pipeline from raw data\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict from raw data. Pipeline will be in charge of transformations\n",
    "y_predict = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uib",
   "language": "python",
   "name": "uib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
