{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML\n",
    "## Selección de modelo y ajuste de hiper-parametros\n",
    "---\n",
    "\n",
    "En la sesión anterior vimos cómo usar la librería *de facto* para machine learning en python: `sklearn`.\n",
    "\n",
    "En la sesión de hoy hablaremos de como las funcionalidades avanzadas de `sklearn` nos facilitan tanto la elección de modelos cómo su validación.\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class='panel-heading'>\n",
    "    <h4>Empecemos</h4>\n",
    "    </div>\n",
    "    <div class='panel-body'>\n",
    "    <ol type=\"A\">\n",
    "    <li>Selección de Modelos</li>\n",
    "    <li>Ajuste de hiper-parámetros</li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selección de Modelos\n",
    "---\n",
    "\n",
    "Los algoritmos de ML son herramientas muy potentes que nos permiten tanto ganar información no accesible a primera vista y, además, realizar modelos sobre los datos de forma que podamos hacer predicciones sobre nuevas observaciones.\n",
    "\n",
    "A la hora de encarar un modelado predictivo, tenemos a nuestro haber un abanico muy amplio de modelos sobre los que elegir. La pregunta es: ¿cómo elijo un modelo?\n",
    "\n",
    "### 1.1 Bies y varianza\n",
    "\n",
    "Cómo bien sabéis a estas alturas, si nos dan una serie de atributos (*features*) $X = x_1, ..., x_n$ y un objetivo asociado (*target*) $y = y_1, ...y_n$, existe una función **real** $f(x)$ que nos permite modelar los datos de forma que $y = f(x) + \\epsilon$, dónde $\\epsilon$ es ruido con $\\mu = 0$ y $var = \\sigma^2$\n",
    "\n",
    "Lo que queremos conseguir es un modelo, al que llamaremos $\\hat{f}(x)$ que **aproxime** la función real lo mejor posible. Sucede que independientemente del modelo de ML que seleccionemos para aproximarnos a la función **real** podemos descomponer el **error** que comete nuestro modelo $\\hat{y}$ sobre nuevas observaciones $x_{test}$ en tres componentes distintas:\n",
    "\n",
    "$ Err(x) = \\mathbb{E} \\left[ \\left( y_{test} - \\hat{y} \\right)^2 \\right] = \\left(E[\\hat{f}(x)]-f(x)\\right)^2 + E\\left[\\left(\\hat{f}(x)-E[\\hat{f}(x)]\\right)^2\\right] +\\sigma_e^2$\n",
    "\n",
    "* **Bias (sesgo)**\n",
    "\n",
    "$E[\\hat{f}(x)]-f(x)$\n",
    "\n",
    "La diferencia entre el valor predicho por el modelo y el valor real.\n",
    "\n",
    "Respecto al modelado, puede interpretarse como los supuestos de simplificación que hace un modelo para facilitar el aprendizaje de la función objetivo.\n",
    "\n",
    "* Low bias: Pocos supuestos sobre la forma de los datos, más flexibles: Decission trees, knn, ...\n",
    "* High bias: Simplificaciones sobre la forma de los datos, menos flexibles: Linear models\n",
    "\n",
    "\n",
    "* **Varianza**\n",
    "\n",
    "$E\\left[\\left(\\hat{f}(x)-E[\\hat{f}(x)]\\right)^2\\right]$\n",
    "\n",
    "El error producido debido a la sensibilidad del modelo con respecto a los datos de entreno. Imagina que de un set de datos extraemos de forma aleatoria dos subgrupos de entreno. Idealmente, las predicciones del modelo no deberían cambiar mucho de un set de entreno a otro. Esto significaría que el modelo que hemos entrenado está haciendo un buen trabajo haciendo el mapeo de las variables de entrada a y salida.\n",
    "\n",
    "Podemos dividir los distintos algoritmos de ML segun como le afectan los cambios en los datos de entreno:\n",
    "\n",
    "* Low Variance: Pequeños cambios en las estimaciones predichas: Linear models\n",
    "* High Variance: Grandes cambios en las estimaciones predichas: Decission trees, knn, ...\n",
    "\n",
    "### 1.2 Compromiso entre bies y varianza\n",
    "\n",
    "Como hemos visto, los modelos paramétricos, tipo los lineales, son modelos con un alto bies y una baja varianza. Por otro lado, los algoritmos no paramétricos, tipo los arboles de decision, son modelos con bajo bies y alta varianza.\n",
    "\n",
    "Existe un problema de modelado referente a cada uno de los casos. En el primero, corremos el riesgo de so-entreno (underfitting), es decir, que nuestro modelo no sea lo suficientemente flexible como para poder representar la función real que intentamos aproximar. En el segundo caso, nos encontramos en el polo opuesto: nuestro modelo es tan flexible que en vez de aproximarse a la función real se \"aprende\" el set de datos de entreno de memoria, por lo que impide la generalización del modelo fuera del set de entreno.\n",
    "\n",
    "<img src='img/underfitting_overfitting.png'>\n",
    "\n",
    "<img src='img/bias-variance.png'>\n",
    "\n",
    "Vemos en el gráfico de arriba que un buen modelo es aquel que nos minimiza el error de las predicciones y que este óptimo implica un compromiso entre lo simple o complejo que es mi modelo y los datos que quiero representar.\n",
    "\n",
    "Entonces, ¿cómo prevenimos que nuestro modelo se quede corto o bien no generalice de forma óptima a nuevas observaciones?\n",
    "\n",
    "La única forma de comprobar que nuestro modelo generaliza bien es guardando una parte de los datos para medir el error *out-of-sample*, es decir, el error sobre los datos que no ha visto el modelo durante la fase de entreno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del entorno\n",
    "---\n",
    "**Ejercicio 0: Cómo siempre, empezaremos por cargar las librerías necesarias para poder trabajar con datos en python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as ##\n",
    "import numpy as ##\n",
    "\n",
    "## figure aesthetics\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1: Carga el dataset de ejemplo por defecto de las casas de boston**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.get('DESCR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crea un dataframe con la información**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "column_names = data.feature_names\n",
    "\n",
    "y = data.target\n",
    "\n",
    "df = pd.DataFrame(##,\n",
    "                  columns=##\n",
    "                 )\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Muestra las primeras 5 lineas del df\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Para ilustrar de forma práctica el concepto de overfitting y el compromiso entre el bies y la varianza, vamos a simplificar el modelado usando solo la variable `LSTAT` y dos modelos distintos que representen a los dos extremos de los puntos anteriores:\n",
    "\n",
    "* Bajo bies y alta varianza: ???\n",
    "\n",
    "* Alto bies y baja varianza: ???\n",
    "\n",
    "Antes de empezar, vamos a ver de forma clara como luce nuestro problema de una sola variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5: Haz un grafico con la relación de la variable objetivo `y` con el único predictor, `LSTAT`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6: Divide el dataset en dos sub datasets, el de entreno y el de testeo con el 10% del tamaño total del dataset. Haz un grafico como el anterior con los diferentes datasets con colores distintos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(##,\n",
    "                                     test_size= ##,\n",
    "                                     random_state=42)\n",
    "\n",
    "sns.scatterplot(x='LSTAT', y='y', data=df_train, label='train')\n",
    "sns.scatterplot(x='LSTAT', y='y', data=df_test, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7: Importa de la libreria sklearn los modelos elegidos anteriormente y entrenalos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ##\n",
    "from sklearn.linear_model import ##\n",
    "\n",
    "# Instancia las clases de los modelos importados\n",
    "reg_DT = ##\n",
    "reg_LR = ##\n",
    "\n",
    "## Define las variables de entrenamiento y la variable objetivo\n",
    "feat_cols_list = ##\n",
    "X_train = df_train[feat_cols]\n",
    "y_train = df_train['y']\n",
    "\n",
    "## Entrena los modelos\n",
    "\n",
    "reg_DT.##\n",
    "reg_LR.##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 8: Evalúa el error producido dentro de la muestra de entreno para los dos algoritmos. Utiliza el RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "y_hat_train_DT = ##\n",
    "y_hat_train_LR = ##\n",
    "\n",
    "in_sample_error_DT = RMSE(y_train, y_hat_train_DT)\n",
    "in_sample_error_LR = ##\n",
    "\n",
    "print('In sample error for DT: {e} miles de $'.format(e=round(in_sample_error_DT, 2)))\n",
    "print('In sample error for LR: {e} miles de $'.format(e= round(in_sample_error_LR, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 9: Evalúa el error producido fuera de la muestra de entreno (datos de test) para los dos algoritmos. Utiliza el RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ##\n",
    "y_test = ##\n",
    "\n",
    "y_hat_test_DT = ##\n",
    "y_hat_test_LR = ##\n",
    "\n",
    "out_sample_error_DT = ##\n",
    "out_sample_error_LR = ##\n",
    "\n",
    "print('Out of sample error for DT: {e} miles de $'.format(e=round(out_sample_error_DT, 2)))\n",
    "print('Out of sample error for LR: {e} miles de $'.format(e= round(out_sample_error_LR, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point\n",
    "\n",
    "Qué esta pasando?\n",
    "\n",
    "Parece que un modelo simple de regresión lineal generaliza mejor que un modelo con un bies bajo? Cómo puede ser esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 10: Haz una predicción para todos los valores que pueden tomar la variable de predicción** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula el valor minimo del campo LSTAT\n",
    "x_min = ##\n",
    "\n",
    "# calcula el valor máximo del campo LSTAT\n",
    "x_max = ##\n",
    "\n",
    "# Crea un array de 200 elementos que vayan de manera uniforme entre los límites de LSTAT.\n",
    "x = np.linspace(##, ##, 200)\n",
    "\n",
    "# Haz un reshape de los valores usando .reshape(-1, 1). Nos evitará problemas con el resto de la librería.\n",
    "x = x.reshape(-1, 1)\n",
    "\n",
    "# Haz una predicción para los valores del array creado en el paso anterior para cada uno de los modelos\n",
    "y_hat_DT = reg_DT.predict(x)\n",
    "y_hat_LR = reg_LR.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a graficar los valores de entreno y test con las predicciones de cada uno de los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un subplot de dos columnas con un tamaño de 12 x 6\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "# Asigna al primer subplot los datos de entreno contra las predicciones de los dos modelos\n",
    "ax[0].scatter(X_train, y_train, label='train', c='firebrick', alpha=0.5)\n",
    "ax[0].plot(x, y_hat_DT, '*-', label='DT')\n",
    "ax[0].plot(x, y_hat_LR, c='orange', label = 'LR')\n",
    "ax[0].legend()\n",
    "\n",
    "# Asigna al segundo subplot los datos de testeo contra las predicciones de los dos modelos\n",
    "ax[1].scatter(X_test, y_test, label='test', c='green', alpha=0.5)\n",
    "ax[1].plot(x, y_hat_DT, 'X-', label='DT')\n",
    "ax[1].plot(x, y_hat_LR,  c='orange', label = 'LR')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, lo que estamos observando aquí en directo es nada más ni nada menos que los efectos de *sub* y *sobre* entreno.\n",
    "\n",
    "El arbol de decisión, al irse creando a medida que se entrena sobre los datos, se adapta demasiado bien a los datos de entreno y generaliza mal. En otras palabras, al aumentar su flexibilidad ha ido disminuyendo el error de *bies*, pero ha ido aumentando el error de *varianza*. Al aumentar el error de varianza es cuando los modelos generalizan mal, por que estan desmasiado adaptados a los datos de entrenamiento.\n",
    "\n",
    "En el caso de los arboles de decision, podemos limitar su flexibilidad para evitar aumentar el error de varianza en exceso.\n",
    "\n",
    "Uno de los hiper-parametros que nos permite \"jugar\" con la flexibilidad del modelo es la profundidad (max_depth). Vamos a echarle un vistazo más cercano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 12: Compara gráficamente como evoluciona el RMSE en función de la profundidad del arbol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = range(1, 20)\n",
    "in_sample_errors = []\n",
    "out_sample_errors = []\n",
    "for max_depth in max_depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=##).fit(##, ##)\n",
    "    y_pred_train = tree.predict(X_train)\n",
    "    y_pred_test = tree.predict(X_test)\n",
    "    in_sample_error = ##\n",
    "    out_sample_error = ##\n",
    "    in_sample_errors.append(in_sample_error)\n",
    "    out_sample_errors.append(out_sample_error)\n",
    "\n",
    "plt.plot(max_depths, in_sample_errors, c='firebrick', label='In-Sample Error')\n",
    "plt.plot(max_depths, out_sample_errors, label='Out-Sample Error')\n",
    "\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica de arriba, observamos como a partir de una profundidad de 4-5 el error `in-sample`, el que cometemos sobre el data set de entrenamiento continua disminuyendo. Estamos reduciendo error de bies. Sin embargo, el error `out-sample` empieza a aumentar a consecuencia de aumentar el error de varianza.\n",
    "\n",
    "El punto óptimo, es conseguir el modelo que consigue minimizar el maximo de bies sin aumentar el error de varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación cruzada (cross-validation)\n",
    "---\n",
    "Ya hemos visto que el mejor modelo no es aquel que minimiza el error dentro de la muestra de entreno, si no que es aquel que minimiza el error de la **muestra de testeo**.\n",
    "\n",
    "A pesar de que la división entre datos de entreno y datos de testeo da buenos resultados, es una aproximación básica que no nos permite afinar los hiperparametros de forma correcta. A base de afinar los hiperparametros evaluando el modelo en el set de testeo, podemos acabar incurriendon en una especie de \"filtración\" de la información de los datos de testeo al modelo. Con tal de evitar esto, en vez de dividir sólo en datos de entreno y testeo, deberíamos dividirlos en 3 partes: entreno, validación y test. De esta forma, podemos entrenar y refinar sobre los datos de entreno y validación para que una vez tengamos el modelo \"afinado\" podamos testearlo en el set de test.\n",
    "\n",
    "<img src='img/trainvalidationtest.jpg'>\n",
    "\n",
    "Esta aproximación, a pesar de ser correcta, implica ciertas limitaciones:\n",
    "\n",
    "1. Reduce drásticamente el volumen de datos con el que estamos trabajando\n",
    "1. No permite hacer \"estadística\" sobre los valores que obtenemos\n",
    "\n",
    "Una forma de hacer frente a estas limitaciónes es mediante la técnica conocida cómo **validación cruzada**. Consiste en dividir el set de datos en $k$ partes iguales. Una vez hecho esto, podemos entrenar el modelo en $k-1$ y testear en aquella que hemos reservado. A esto se le llama *k-fold cross validation*. Entre sus variantes encontramos:\n",
    "\n",
    "- **Stratified K-Fold Validation**: cada sub muestra mantiene la proporción de y's (clasificación)\n",
    "- **Leave-One-Out**: Si $k=n$, dónde $n$ es el número de muestras que tenemos. Intensivo a nivel computacional\n",
    "\n",
    "<img src='img/kfold.png'>\n",
    "\n",
    "Echale un vistazo a la [documentación](http://scikit-learn.org/stable/modules/cross_validation.html)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DT_scores = cross_val_score(estimator=##,\n",
    "                            X=##,\n",
    "                            y=#,\n",
    "                            cv=#,\n",
    "                            scoring='neg_root_mean_squared_error') # Puedes consultar la lista de metrics para escorizar el modelo --> from sklearn.metrics import SCORERS; SCORERS.keys()\n",
    "\n",
    "print('RMSE for DT: {m} +- {s}'.format(m=-round(DT_scores.mean(), 2),\n",
    "                                       s=round(DT_scores.std(),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 13: Calcula el RMSE medio para la regresión lineal usando un k=5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_scores = cross_val_score(estimator=##,\n",
    "                            X=##,\n",
    "                            y=##,\n",
    "                            cv=##,\n",
    "                            scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print('RMSE for LR: {m} +- {s}'.format(m=-round(LR_scores.mean(), 2),\n",
    "                                       s=round(LR_scores.std(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 14: Grafica cómo evoluciona el RMSE medio para k=10, en comparación con los calculados en el ejercicio 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializa una lista vacia\n",
    "cv_test_errors_mean = []\n",
    "cv_test_errors_list = []\n",
    "\n",
    "#itera sobre la lista de max_depths del ejercicio 12\n",
    "for max_depth in max_depths:\n",
    "    # Instancia el arbol\n",
    "    tree = DecisionTreeRegressor(max_depth=##)\n",
    "    \n",
    "    # calcula el error usando cross_val_score y todo el set de datos.\n",
    "    # Recuerda en negar el resultado\n",
    "    cv_test_error = -cross_val_score(##,\n",
    "                                     X=##,\n",
    "                                     y=##,\n",
    "                                     cv=5,\n",
    "                                     scoring='neg_root_mean_squared_error')\n",
    "\n",
    "    #calcula el valor medio y haz un append a la lista\n",
    "    cv_test_errors_mean.append(cv_test_error.mean())\n",
    "    \n",
    "    # haz un append a la lista con todo el array de errores\n",
    "    cv_test_errors_list.append(cv_test_error)\n",
    "\n",
    "plt.plot(max_depths, in_sample_errors, c='firebrick', label='In-Sample Error')\n",
    "plt.plot(max_depths, out_sample_errors, label='Out-Sample Error')\n",
    "plt.plot(max_depths, cv_test_errors_mean, label='Cross-Validation Error')\n",
    "plt.legend(loc='best')\n",
    "                                 \n",
    "errors = pd.DataFrame(cv_test_errors_list)                                 \n",
    "for e in errors:\n",
    "    plt.plot(max_depths, errors[e], c='gray', alpha=0.2)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search: ajuste de hiper-parametros.\n",
    "---\n",
    "Como hemos visto en el apartado anterior, podemos ajustar los hiper-parametros de ciertos modelos para aumentar o disminuir su \"flexibilidad\" a la hora de adaptarse a los datos y seleccionar el mejor a partir del resultado de la validación cruzada. Si repitieramos el ejercicio anterior para cada uno de los hiper-parametros del modelo podriamos afinar el modelo para poder exprimirlo completamente.\n",
    "\n",
    "Podemos automátizar este proceso por fuerza bruta definiendo un espacio de busqueda sobre los distintos hiper-parametros que queremos probar y calculando el error cométido para cada una de las combinaciones.\n",
    "\n",
    "Sklearn nos da una herramienta muy útil para tal propósito, GridSearchCV. Esta herramienta evaluará el modelo con cada una de las combinaciones de hiper-parametros utilizando la técnica del cross validation.\n",
    "\n",
    "Veamos como usarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_depth': ##, crea una lista con los posibles valores a probar de profundidad\n",
    "          'min_samples_split': ##, crea una lista con los posibles valores a probar\n",
    "          'min_samples_leaf': ## crea una lista con los posibles valores a probar\n",
    "         }\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "gs_reg_DT = GridSearchCV(estimator=##,\n",
    "                         param_grid=params,\n",
    "                         cv=5,\n",
    "                         scoring='neg_root_mean_squared_error')\n",
    "\n",
    "gs_reg_DT.fit(df[['LSTAT']].values, df.y)\n",
    "print(gs_reg_DT.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 15: Guarda en un dataframe los resultados de las iteraciones.**\n",
    "\n",
    "Los encontraras en `gs_reg_DT.cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(gs_reg_DT.cv_results_)\n",
    "\n",
    "df_cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 16: Compara graficamente el mejor modelo DT con la regresion lineal y el arbol de decision iniciales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, c='gray', alpha=0.5)\n",
    "plt.plot(x, reg_LR.predict(x), c ='green', label='Linear Reg', alpha=0.5)\n",
    "plt.plot(x, reg_DT.predict(x), c ='blue', label='Overfit DT', alpha=0.5)\n",
    "plt.plot(x, gs_reg_DT.predict(x), c='firebrick', label='Fine tuned Decission Tree', alpha=0.5)\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
